[{"category":"Operations","homepage_url":"https://github.com/dusty-nv/jetson-inference","id":"operations--model-serving--dusty-nv-jetson-inference","logo_url":"http://127.0.0.1:8000/logos/0b22fe6553ffd6c927de0a316a0b01cde01e4c3932170c13bfebb45153f0cdb8.svg","name":"dusty-nv/jetson-inference","subcategory":"Model Serving","description":"Hello AI World guide to deploying deep-learning inference networks and deep vision primitives with TensorRT and NVIDIA Jetson.","repositories":[{"url":"https://github.com/dusty-nv/jetson-inference","primary":true}]},{"category":"Operations","homepage_url":"https://github.com/horseee/LLM-Pruner","id":"operations--model-serving--horseee-llm-pruner","logo_url":"http://127.0.0.1:8000/logos/0b22fe6553ffd6c927de0a316a0b01cde01e4c3932170c13bfebb45153f0cdb8.svg","name":"horseee/LLM-Pruner","subcategory":"Model Serving","description":"[NeurIPS 2023] LLM-Pruner: On the Structural Pruning of Large Language Models. Support Llama-3/3.1, Llama-2, LLaMA,  BLOOM, Vicuna, Baichuan, TinyLlama, etc.","repositories":[{"url":"https://github.com/horseee/LLM-Pruner","primary":true}]},{"category":"Operations","homepage_url":"https://github.com/huggingface/text-embeddings-inference","id":"operations--model-serving--huggingface-text-embeddings-inference","logo_url":"http://127.0.0.1:8000/logos/0b22fe6553ffd6c927de0a316a0b01cde01e4c3932170c13bfebb45153f0cdb8.svg","name":"huggingface/text-embeddings-inference","subcategory":"Model Serving","description":"A blazing fast inference solution for text embeddings models","repositories":[{"url":"https://github.com/huggingface/text-embeddings-inference","primary":true}]},{"category":"Operations","homepage_url":"https://github.com/huggingface/text-generation-inference","id":"operations--model-serving--huggingface-text-generation-inference","logo_url":"http://127.0.0.1:8000/logos/0b22fe6553ffd6c927de0a316a0b01cde01e4c3932170c13bfebb45153f0cdb8.svg","name":"huggingface/text-generation-inference","subcategory":"Model Serving","description":"Large Language Model Text Generation Inference","repositories":[{"url":"https://github.com/huggingface/text-generation-inference","primary":true}]},{"category":"Operations","homepage_url":"https://github.com/janhq/jan","id":"operations--model-serving--janhq-jan","logo_url":"http://127.0.0.1:8000/logos/0b22fe6553ffd6c927de0a316a0b01cde01e4c3932170c13bfebb45153f0cdb8.svg","name":"janhq/jan","subcategory":"Model Serving","description":"Jan is an open source alternative to ChatGPT that runs 100% offline on your computer. Multiple engine support (llama.cpp, TensorRT-LLM)","repositories":[{"url":"https://github.com/janhq/jan","primary":true}]},{"category":"Operations","homepage_url":"https://github.com/jmorganca/ollama","id":"operations--model-serving--jmorganca-ollama","logo_url":"http://127.0.0.1:8000/logos/0b22fe6553ffd6c927de0a316a0b01cde01e4c3932170c13bfebb45153f0cdb8.svg","name":"jmorganca/ollama","subcategory":"Model Serving","description":"Get up and running with Llama 3.2, Mistral, Gemma 2, and other large language models.","repositories":[{"url":"https://github.com/jmorganca/ollama","primary":true}]},{"category":"Operations","homepage_url":"https://github.com/jncraton/languagemodels","id":"operations--model-serving--jncraton-languagemodels","logo_url":"http://127.0.0.1:8000/logos/0b22fe6553ffd6c927de0a316a0b01cde01e4c3932170c13bfebb45153f0cdb8.svg","name":"jncraton/languagemodels","subcategory":"Model Serving","description":"Explore large language models in 512MB of RAM","repositories":[{"url":"https://github.com/jncraton/languagemodels","primary":true}]},{"category":"Operations","homepage_url":"https://github.com/kuleshov/minillm","id":"operations--model-serving--kuleshov-minillm","logo_url":"http://127.0.0.1:8000/logos/0b22fe6553ffd6c927de0a316a0b01cde01e4c3932170c13bfebb45153f0cdb8.svg","name":"kuleshov/minillm","subcategory":"Model Serving","description":"MiniLLM is a minimal system for running modern LLMs on consumer-grade GPUs","repositories":[{"url":"https://github.com/kuleshov/minillm","primary":true}]},{"category":"Operations","homepage_url":"https://github.com/labmlai/labml","id":"operations--monitoring--labmlai-labml","logo_url":"http://127.0.0.1:8000/logos/0b22fe6553ffd6c927de0a316a0b01cde01e4c3932170c13bfebb45153f0cdb8.svg","name":"labmlai/labml","subcategory":"Monitoring","description":"üîé Monitor deep learning model training and hardware usage from your mobile phone üì±","repositories":[{"url":"https://github.com/labmlai/labml","primary":true}]},{"category":"Operations","homepage_url":"https://github.com/langfuse/langfuse","id":"operations--monitoring--langfuse-langfuse","logo_url":"http://127.0.0.1:8000/logos/0b22fe6553ffd6c927de0a316a0b01cde01e4c3932170c13bfebb45153f0cdb8.svg","name":"langfuse/langfuse","subcategory":"Monitoring","description":"ü™¢ Open source LLM engineering platform: LLM Observability, metrics, evals, prompt management, playground, datasets. Integrates with LlamaIndex, Langchain, OpenAI SDK, LiteLLM, and more. üçäYC W23 ","repositories":[{"url":"https://github.com/langfuse/langfuse","primary":true}]},{"category":"Operations","homepage_url":"https://github.com/mit-han-lab/llm-awq","id":"operations--model-serving--mit-han-lab-llm-awq","logo_url":"http://127.0.0.1:8000/logos/0b22fe6553ffd6c927de0a316a0b01cde01e4c3932170c13bfebb45153f0cdb8.svg","name":"mit-han-lab/llm-awq","subcategory":"Model Serving","description":"[MLSys 2024 Best Paper Award] AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration","repositories":[{"url":"https://github.com/mit-han-lab/llm-awq","primary":true}]},{"category":"Operations","homepage_url":"https://github.com/mit-han-lab/smoothquant","id":"operations--model-serving--mit-han-lab-smoothquant","logo_url":"http://127.0.0.1:8000/logos/0b22fe6553ffd6c927de0a316a0b01cde01e4c3932170c13bfebb45153f0cdb8.svg","name":"mit-han-lab/smoothquant","subcategory":"Model Serving","description":"[ICML 2023] SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models","repositories":[{"url":"https://github.com/mit-han-lab/smoothquant","primary":true}]},{"category":"Operations","homepage_url":"https://github.com/nebuly-ai/nebuly","id":"operations--monitoring--nebuly-ai-nebuly","logo_url":"http://127.0.0.1:8000/logos/0b22fe6553ffd6c927de0a316a0b01cde01e4c3932170c13bfebb45153f0cdb8.svg","name":"nebuly-ai/nebuly","subcategory":"Monitoring","description":"A collection of libraries to optimise AI model performances","repositories":[{"url":"https://github.com/nebuly-ai/nebuly","primary":true}]},{"category":"Operations","homepage_url":"https://github.com/neuralmagic/deepsparse","id":"operations--model-serving--neuralmagic-deepsparse","logo_url":"http://127.0.0.1:8000/logos/0b22fe6553ffd6c927de0a316a0b01cde01e4c3932170c13bfebb45153f0cdb8.svg","name":"neuralmagic/deepsparse","subcategory":"Model Serving","description":"Sparsity-aware deep learning inference runtime for CPUs","repositories":[{"url":"https://github.com/neuralmagic/deepsparse","primary":true}]},{"category":"Operations","homepage_url":"https://github.com/pytorch-labs/gpt-fast","id":"operations--model-serving--pytorch-labs-gpt-fast","logo_url":"http://127.0.0.1:8000/logos/0b22fe6553ffd6c927de0a316a0b01cde01e4c3932170c13bfebb45153f0cdb8.svg","name":"pytorch-labs/gpt-fast","subcategory":"Model Serving","description":"Simple and efficient pytorch-native transformer text generation in <1000 LOC of python.","repositories":[{"url":"https://github.com/pytorch-labs/gpt-fast","primary":true}]},{"category":"Operations","homepage_url":"https://github.com/TimDettmers/bitsandbytes","id":"operations--model-serving--timdettmers-bitsandbytes","logo_url":"http://127.0.0.1:8000/logos/0b22fe6553ffd6c927de0a316a0b01cde01e4c3932170c13bfebb45153f0cdb8.svg","name":"TimDettmers/bitsandbytes","subcategory":"Model Serving","description":"Accessible large language models via k-bit quantization for PyTorch.","repositories":[{"url":"https://github.com/TimDettmers/bitsandbytes","primary":true}]}]