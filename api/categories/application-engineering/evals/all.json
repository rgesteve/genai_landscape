[{"category":"Application Engineering","homepage_url":"https://github.com/Azure/PyRIT","id":"application-engineering--evals--azure-pyrit","logo_url":"http://127.0.0.1:8000/logos/0b22fe6553ffd6c927de0a316a0b01cde01e4c3932170c13bfebb45153f0cdb8.svg","name":"Azure/PyRIT","subcategory":"Evals","description":"The Python Risk Identification Tool for generative AI (PyRIT) is an open access automation framework to empower security professionals and machine learning engineers to proactively find risks in their generative AI systems.","repositories":[{"url":"https://github.com/Azure/PyRIT","primary":true}]},{"category":"Application Engineering","homepage_url":"https://github.com/confident-ai/deepeval","id":"application-engineering--evals--confident-ai-deepeval","logo_url":"http://127.0.0.1:8000/logos/0b22fe6553ffd6c927de0a316a0b01cde01e4c3932170c13bfebb45153f0cdb8.svg","name":"confident-ai/deepeval","subcategory":"Evals","description":"The LLM Evaluation Framework","repositories":[{"url":"https://github.com/confident-ai/deepeval","primary":true}]},{"category":"Application Engineering","homepage_url":"https://github.com/embeddings-benchmark/mteb","id":"application-engineering--evals--embeddings-benchmark-mteb","logo_url":"http://127.0.0.1:8000/logos/0b22fe6553ffd6c927de0a316a0b01cde01e4c3932170c13bfebb45153f0cdb8.svg","name":"embeddings-benchmark/mteb","subcategory":"Evals","description":"MTEB: Massive Text Embedding Benchmark","repositories":[{"url":"https://github.com/embeddings-benchmark/mteb","primary":true}]},{"category":"Application Engineering","homepage_url":"https://github.com/huggingface/evaluate","id":"application-engineering--evals--huggingface-evaluate","logo_url":"http://127.0.0.1:8000/logos/0b22fe6553ffd6c927de0a316a0b01cde01e4c3932170c13bfebb45153f0cdb8.svg","name":"huggingface/evaluate","subcategory":"Evals","description":"ðŸ¤— Evaluate: A library for easily evaluating machine learning models and datasets.","repositories":[{"url":"https://github.com/huggingface/evaluate","primary":true}]},{"category":"Application Engineering","homepage_url":"https://github.com/openai/grade-school-math","id":"application-engineering--evals--openai-grade-school-math","logo_url":"http://127.0.0.1:8000/logos/0b22fe6553ffd6c927de0a316a0b01cde01e4c3932170c13bfebb45153f0cdb8.svg","name":"openai/grade-school-math","subcategory":"Evals","repositories":[{"url":"https://github.com/openai/grade-school-math","primary":true}]},{"category":"Application Engineering","homepage_url":"https://github.com/openai/prm800k","id":"application-engineering--evals--openai-prm800k","logo_url":"http://127.0.0.1:8000/logos/0b22fe6553ffd6c927de0a316a0b01cde01e4c3932170c13bfebb45153f0cdb8.svg","name":"openai/prm800k","subcategory":"Evals","description":"800,000 step-level correctness labels on LLM solutions to MATH problems","repositories":[{"url":"https://github.com/openai/prm800k","primary":true}]},{"category":"Application Engineering","homepage_url":"https://github.com/rlancemartin/auto-evaluator","id":"application-engineering--evals--rlancemartin-auto-evaluator","logo_url":"http://127.0.0.1:8000/logos/0b22fe6553ffd6c927de0a316a0b01cde01e4c3932170c13bfebb45153f0cdb8.svg","name":"rlancemartin/auto-evaluator","subcategory":"Evals","description":"Evaluation tool for LLM QA chains","repositories":[{"url":"https://github.com/rlancemartin/auto-evaluator","primary":true}]},{"category":"Application Engineering","homepage_url":"https://github.com/THUDM/AgentBench","id":"application-engineering--evals--thudm-agentbench","logo_url":"http://127.0.0.1:8000/logos/0b22fe6553ffd6c927de0a316a0b01cde01e4c3932170c13bfebb45153f0cdb8.svg","name":"THUDM/AgentBench","subcategory":"Evals","description":"A Comprehensive Benchmark to Evaluate LLMs as Agents (ICLR'24)","repositories":[{"url":"https://github.com/THUDM/AgentBench","primary":true}]}]