{"classification":{"category":{"name":"Models","normalized_name":"models","subcategories":[{"name":"Fine tuning","normalized_name":"fine-tuning"},{"name":"Foundation model","normalized_name":"foundation-model"}]}},"foundation":"DEMO","items":[{"category":"Models","id":"models--fine-tuning--huggingface-peft","name":"huggingface/peft","logo":"logos/0b22fe6553ffd6c927de0a316a0b01cde01e4c3932170c13bfebb45153f0cdb8.svg","subcategory":"Fine tuning","website":"https://github.com/huggingface/peft","description":"ü§ó PEFT: State-of-the-art Parameter-Efficient Fine-Tuning.","primary_repository_url":"https://github.com/huggingface/peft"},{"category":"Models","id":"models--fine-tuning--eric-mitchell-direct-preference-optimization","name":"eric-mitchell/direct-preference-optimization","logo":"logos/0b22fe6553ffd6c927de0a316a0b01cde01e4c3932170c13bfebb45153f0cdb8.svg","subcategory":"Fine tuning","website":"https://github.com/eric-mitchell/direct-preference-optimization","description":"Reference implementation for DPO (Direct Preference Optimization)","primary_repository_url":"https://github.com/eric-mitchell/direct-preference-optimization"},{"category":"Models","id":"models--fine-tuning--allenai-rl4lms","name":"allenai/RL4LMs","logo":"logos/0b22fe6553ffd6c927de0a316a0b01cde01e4c3932170c13bfebb45153f0cdb8.svg","subcategory":"Fine tuning","website":"https://github.com/allenai/RL4LMs","description":"A modular RL library to fine-tune language models to human preferences","primary_repository_url":"https://github.com/allenai/RL4LMs"},{"category":"Models","id":"models--foundation-model--facebookresearch-llama","name":"facebookresearch/llama","logo":"logos/0b22fe6553ffd6c927de0a316a0b01cde01e4c3932170c13bfebb45153f0cdb8.svg","subcategory":"Foundation model","website":"https://github.com/facebookresearch/llama","description":"Inference code for Llama models","primary_repository_url":"https://github.com/facebookresearch/llama"},{"category":"Models","id":"models--foundation-model--qwenlm-qwen","name":"QwenLM/Qwen","logo":"logos/0b22fe6553ffd6c927de0a316a0b01cde01e4c3932170c13bfebb45153f0cdb8.svg","subcategory":"Foundation model","website":"https://github.com/QwenLM/Qwen","description":"The official repo of Qwen (ÈÄö‰πâÂçÉÈóÆ) chat & pretrained large language model proposed by Alibaba Cloud.","primary_repository_url":"https://github.com/QwenLM/Qwen"},{"category":"Models","id":"models--foundation-model--lucidrains-denoising-diffusion-pytorch","name":"lucidrains/denoising-diffusion-pytorch","logo":"logos/0b22fe6553ffd6c927de0a316a0b01cde01e4c3932170c13bfebb45153f0cdb8.svg","subcategory":"Foundation model","website":"https://github.com/lucidrains/denoising-diffusion-pytorch","description":"Implementation of Denoising Diffusion Probabilistic Model in Pytorch","primary_repository_url":"https://github.com/lucidrains/denoising-diffusion-pytorch"},{"category":"Models","id":"models--foundation-model--nvidia-megatron-lm","name":"NVIDIA/Megatron-LM","logo":"logos/0b22fe6553ffd6c927de0a316a0b01cde01e4c3932170c13bfebb45153f0cdb8.svg","subcategory":"Foundation model","website":"https://github.com/NVIDIA/Megatron-LM","description":"Ongoing research training transformer models at scale","primary_repository_url":"https://github.com/NVIDIA/Megatron-LM"}]}