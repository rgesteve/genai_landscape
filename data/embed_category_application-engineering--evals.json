{"classification":{"category":{"name":"Application Engineering","normalized_name":"application-engineering","subcategories":[{"name":"Evals","normalized_name":"evals"}]}},"foundation":"DEMO","items":[{"category":"Application Engineering","id":"application-engineering--evals--confident-ai-deepeval","name":"confident-ai/deepeval","logo":"logos/0b22fe6553ffd6c927de0a316a0b01cde01e4c3932170c13bfebb45153f0cdb8.svg","subcategory":"Evals","website":"https://github.com/confident-ai/deepeval","description":"The LLM Evaluation Framework","primary_repository_url":"https://github.com/confident-ai/deepeval"},{"category":"Application Engineering","id":"application-engineering--evals--huggingface-evaluate","name":"huggingface/evaluate","logo":"logos/0b22fe6553ffd6c927de0a316a0b01cde01e4c3932170c13bfebb45153f0cdb8.svg","subcategory":"Evals","website":"https://github.com/huggingface/evaluate","description":"ðŸ¤— Evaluate: A library for easily evaluating machine learning models and datasets.","primary_repository_url":"https://github.com/huggingface/evaluate"},{"category":"Application Engineering","id":"application-engineering--evals--embeddings-benchmark-mteb","name":"embeddings-benchmark/mteb","logo":"logos/0b22fe6553ffd6c927de0a316a0b01cde01e4c3932170c13bfebb45153f0cdb8.svg","subcategory":"Evals","website":"https://github.com/embeddings-benchmark/mteb","description":"MTEB: Massive Text Embedding Benchmark","primary_repository_url":"https://github.com/embeddings-benchmark/mteb"},{"category":"Application Engineering","id":"application-engineering--evals--openai-grade-school-math","name":"openai/grade-school-math","logo":"logos/0b22fe6553ffd6c927de0a316a0b01cde01e4c3932170c13bfebb45153f0cdb8.svg","subcategory":"Evals","website":"https://github.com/openai/grade-school-math","primary_repository_url":"https://github.com/openai/grade-school-math"},{"category":"Application Engineering","id":"application-engineering--evals--thudm-agentbench","name":"THUDM/AgentBench","logo":"logos/0b22fe6553ffd6c927de0a316a0b01cde01e4c3932170c13bfebb45153f0cdb8.svg","subcategory":"Evals","website":"https://github.com/THUDM/AgentBench","description":"A Comprehensive Benchmark to Evaluate LLMs as Agents (ICLR'24)","primary_repository_url":"https://github.com/THUDM/AgentBench"},{"category":"Application Engineering","id":"application-engineering--evals--rlancemartin-auto-evaluator","name":"rlancemartin/auto-evaluator","logo":"logos/0b22fe6553ffd6c927de0a316a0b01cde01e4c3932170c13bfebb45153f0cdb8.svg","subcategory":"Evals","website":"https://github.com/rlancemartin/auto-evaluator","description":"Evaluation tool for LLM QA chains","primary_repository_url":"https://github.com/rlancemartin/auto-evaluator"},{"category":"Application Engineering","id":"application-engineering--evals--openai-prm800k","name":"openai/prm800k","logo":"logos/0b22fe6553ffd6c927de0a316a0b01cde01e4c3932170c13bfebb45153f0cdb8.svg","subcategory":"Evals","website":"https://github.com/openai/prm800k","description":"800,000 step-level correctness labels on LLM solutions to MATH problems","primary_repository_url":"https://github.com/openai/prm800k"},{"category":"Application Engineering","id":"application-engineering--evals--azure-pyrit","name":"Azure/PyRIT","logo":"logos/0b22fe6553ffd6c927de0a316a0b01cde01e4c3932170c13bfebb45153f0cdb8.svg","subcategory":"Evals","website":"https://github.com/Azure/PyRIT","description":"The Python Risk Identification Tool for generative AI (PyRIT) is an open access automation framework to empower security professionals and machine learning engineers to proactively find risks in their generative AI systems.","primary_repository_url":"https://github.com/Azure/PyRIT"}]}