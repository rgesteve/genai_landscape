{"classification":{"category":{"name":"Operations","normalized_name":"operations","subcategories":[{"name":"Model Serving","normalized_name":"model-serving"}]}},"foundation":"DEMO","items":[{"category":"Operations","id":"operations--model-serving--huggingface-text-generation-inference","name":"huggingface/text-generation-inference","logo":"logos/0b22fe6553ffd6c927de0a316a0b01cde01e4c3932170c13bfebb45153f0cdb8.svg","subcategory":"Model Serving","website":"https://github.com/huggingface/text-generation-inference","description":"Large Language Model Text Generation Inference","primary_repository_url":"https://github.com/huggingface/text-generation-inference"},{"category":"Operations","id":"operations--model-serving--huggingface-text-embeddings-inference","name":"huggingface/text-embeddings-inference","logo":"logos/0b22fe6553ffd6c927de0a316a0b01cde01e4c3932170c13bfebb45153f0cdb8.svg","subcategory":"Model Serving","website":"https://github.com/huggingface/text-embeddings-inference","description":"A blazing fast inference solution for text embeddings models","primary_repository_url":"https://github.com/huggingface/text-embeddings-inference"},{"category":"Operations","id":"operations--model-serving--janhq-jan","name":"janhq/jan","logo":"logos/0b22fe6553ffd6c927de0a316a0b01cde01e4c3932170c13bfebb45153f0cdb8.svg","subcategory":"Model Serving","website":"https://github.com/janhq/jan","description":"Jan is an open source alternative to ChatGPT that runs 100% offline on your computer. Multiple engine support (llama.cpp, TensorRT-LLM)","primary_repository_url":"https://github.com/janhq/jan"},{"category":"Operations","id":"operations--model-serving--timdettmers-bitsandbytes","name":"TimDettmers/bitsandbytes","logo":"logos/0b22fe6553ffd6c927de0a316a0b01cde01e4c3932170c13bfebb45153f0cdb8.svg","subcategory":"Model Serving","website":"https://github.com/TimDettmers/bitsandbytes","description":"Accessible large language models via k-bit quantization for PyTorch.","primary_repository_url":"https://github.com/TimDettmers/bitsandbytes"},{"category":"Operations","id":"operations--model-serving--horseee-llm-pruner","name":"horseee/LLM-Pruner","logo":"logos/0b22fe6553ffd6c927de0a316a0b01cde01e4c3932170c13bfebb45153f0cdb8.svg","subcategory":"Model Serving","website":"https://github.com/horseee/LLM-Pruner","description":"[NeurIPS 2023] LLM-Pruner: On the Structural Pruning of Large Language Models. Support Llama-3/3.1, Llama-2, LLaMA,  BLOOM, Vicuna, Baichuan, TinyLlama, etc.","primary_repository_url":"https://github.com/horseee/LLM-Pruner"},{"category":"Operations","id":"operations--model-serving--kuleshov-minillm","name":"kuleshov/minillm","logo":"logos/0b22fe6553ffd6c927de0a316a0b01cde01e4c3932170c13bfebb45153f0cdb8.svg","subcategory":"Model Serving","website":"https://github.com/kuleshov/minillm","description":"MiniLLM is a minimal system for running modern LLMs on consumer-grade GPUs","primary_repository_url":"https://github.com/kuleshov/minillm"},{"category":"Operations","id":"operations--model-serving--jmorganca-ollama","name":"jmorganca/ollama","logo":"logos/0b22fe6553ffd6c927de0a316a0b01cde01e4c3932170c13bfebb45153f0cdb8.svg","subcategory":"Model Serving","website":"https://github.com/jmorganca/ollama","description":"Get up and running with Llama 3.2, Mistral, Gemma 2, and other large language models.","primary_repository_url":"https://github.com/jmorganca/ollama"},{"category":"Operations","id":"operations--model-serving--mit-han-lab-smoothquant","name":"mit-han-lab/smoothquant","logo":"logos/0b22fe6553ffd6c927de0a316a0b01cde01e4c3932170c13bfebb45153f0cdb8.svg","subcategory":"Model Serving","website":"https://github.com/mit-han-lab/smoothquant","description":"[ICML 2023] SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models","primary_repository_url":"https://github.com/mit-han-lab/smoothquant"},{"category":"Operations","id":"operations--model-serving--pytorch-labs-gpt-fast","name":"pytorch-labs/gpt-fast","logo":"logos/0b22fe6553ffd6c927de0a316a0b01cde01e4c3932170c13bfebb45153f0cdb8.svg","subcategory":"Model Serving","website":"https://github.com/pytorch-labs/gpt-fast","description":"Simple and efficient pytorch-native transformer text generation in <1000 LOC of python.","primary_repository_url":"https://github.com/pytorch-labs/gpt-fast"},{"category":"Operations","id":"operations--model-serving--mit-han-lab-llm-awq","name":"mit-han-lab/llm-awq","logo":"logos/0b22fe6553ffd6c927de0a316a0b01cde01e4c3932170c13bfebb45153f0cdb8.svg","subcategory":"Model Serving","website":"https://github.com/mit-han-lab/llm-awq","description":"[MLSys 2024 Best Paper Award] AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration","primary_repository_url":"https://github.com/mit-han-lab/llm-awq"},{"category":"Operations","id":"operations--model-serving--jncraton-languagemodels","name":"jncraton/languagemodels","logo":"logos/0b22fe6553ffd6c927de0a316a0b01cde01e4c3932170c13bfebb45153f0cdb8.svg","subcategory":"Model Serving","website":"https://github.com/jncraton/languagemodels","description":"Explore large language models in 512MB of RAM","primary_repository_url":"https://github.com/jncraton/languagemodels"},{"category":"Operations","id":"operations--model-serving--dusty-nv-jetson-inference","name":"dusty-nv/jetson-inference","logo":"logos/0b22fe6553ffd6c927de0a316a0b01cde01e4c3932170c13bfebb45153f0cdb8.svg","subcategory":"Model Serving","website":"https://github.com/dusty-nv/jetson-inference","description":"Hello AI World guide to deploying deep-learning inference networks and deep vision primitives with TensorRT and NVIDIA Jetson.","primary_repository_url":"https://github.com/dusty-nv/jetson-inference"},{"category":"Operations","id":"operations--model-serving--neuralmagic-deepsparse","name":"neuralmagic/deepsparse","logo":"logos/0b22fe6553ffd6c927de0a316a0b01cde01e4c3932170c13bfebb45153f0cdb8.svg","subcategory":"Model Serving","website":"https://github.com/neuralmagic/deepsparse","description":"Sparsity-aware deep learning inference runtime for CPUs","primary_repository_url":"https://github.com/neuralmagic/deepsparse"}]}