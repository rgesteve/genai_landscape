{"items":[{"category":"Models","homepage_url":"https://github.com/facebookresearch/llama","id":"models--foundation-model--facebookresearch-llama","logo":"logos/0b22fe6553ffd6c927de0a316a0b01cde01e4c3932170c13bfebb45153f0cdb8.svg","name":"facebookresearch/llama","subcategory":"Foundation model","website":"https://github.com/facebookresearch/llama","description":"Inference code for Llama models","repositories":[{"url":"https://github.com/facebookresearch/llama","primary":true}]},{"category":"Models","homepage_url":"https://github.com/QwenLM/Qwen","id":"models--foundation-model--qwenlm-qwen","logo":"logos/0b22fe6553ffd6c927de0a316a0b01cde01e4c3932170c13bfebb45153f0cdb8.svg","name":"QwenLM/Qwen","subcategory":"Foundation model","website":"https://github.com/QwenLM/Qwen","description":"The official repo of Qwen (通义千问) chat & pretrained large language model proposed by Alibaba Cloud.","repositories":[{"url":"https://github.com/QwenLM/Qwen","primary":true}]},{"category":"Models","homepage_url":"https://github.com/lucidrains/denoising-diffusion-pytorch","id":"models--foundation-model--lucidrains-denoising-diffusion-pytorch","logo":"logos/0b22fe6553ffd6c927de0a316a0b01cde01e4c3932170c13bfebb45153f0cdb8.svg","name":"lucidrains/denoising-diffusion-pytorch","subcategory":"Foundation model","website":"https://github.com/lucidrains/denoising-diffusion-pytorch","description":"Implementation of Denoising Diffusion Probabilistic Model in Pytorch","repositories":[{"url":"https://github.com/lucidrains/denoising-diffusion-pytorch","primary":true}]},{"category":"Models","homepage_url":"https://github.com/NVIDIA/Megatron-LM","id":"models--foundation-model--nvidia-megatron-lm","logo":"logos/0b22fe6553ffd6c927de0a316a0b01cde01e4c3932170c13bfebb45153f0cdb8.svg","name":"NVIDIA/Megatron-LM","subcategory":"Foundation model","website":"https://github.com/NVIDIA/Megatron-LM","description":"Ongoing research training transformer models at scale","repositories":[{"url":"https://github.com/NVIDIA/Megatron-LM","primary":true}]}]}