{"items":[{"category":"Models","homepage_url":"https://github.com/huggingface/peft","id":"models--fine-tuning--huggingface-peft","logo":"logos/0b22fe6553ffd6c927de0a316a0b01cde01e4c3932170c13bfebb45153f0cdb8.svg","name":"huggingface/peft","subcategory":"Fine tuning","website":"https://github.com/huggingface/peft","description":"ü§ó PEFT: State-of-the-art Parameter-Efficient Fine-Tuning.","repositories":[{"url":"https://github.com/huggingface/peft","primary":true}]},{"category":"Models","homepage_url":"https://github.com/eric-mitchell/direct-preference-optimization","id":"models--fine-tuning--eric-mitchell-direct-preference-optimization","logo":"logos/0b22fe6553ffd6c927de0a316a0b01cde01e4c3932170c13bfebb45153f0cdb8.svg","name":"eric-mitchell/direct-preference-optimization","subcategory":"Fine tuning","website":"https://github.com/eric-mitchell/direct-preference-optimization","description":"Reference implementation for DPO (Direct Preference Optimization)","repositories":[{"url":"https://github.com/eric-mitchell/direct-preference-optimization","primary":true}]},{"category":"Models","homepage_url":"https://github.com/allenai/RL4LMs","id":"models--fine-tuning--allenai-rl4lms","logo":"logos/0b22fe6553ffd6c927de0a316a0b01cde01e4c3932170c13bfebb45153f0cdb8.svg","name":"allenai/RL4LMs","subcategory":"Fine tuning","website":"https://github.com/allenai/RL4LMs","description":"A modular RL library to fine-tune language models to human preferences","repositories":[{"url":"https://github.com/allenai/RL4LMs","primary":true}]},{"category":"Models","homepage_url":"https://github.com/facebookresearch/llama","id":"models--foundation-model--facebookresearch-llama","logo":"logos/0b22fe6553ffd6c927de0a316a0b01cde01e4c3932170c13bfebb45153f0cdb8.svg","name":"facebookresearch/llama","subcategory":"Foundation model","website":"https://github.com/facebookresearch/llama","description":"Inference code for Llama models","repositories":[{"url":"https://github.com/facebookresearch/llama","primary":true}]},{"category":"Models","homepage_url":"https://github.com/QwenLM/Qwen","id":"models--foundation-model--qwenlm-qwen","logo":"logos/0b22fe6553ffd6c927de0a316a0b01cde01e4c3932170c13bfebb45153f0cdb8.svg","name":"QwenLM/Qwen","subcategory":"Foundation model","website":"https://github.com/QwenLM/Qwen","description":"The official repo of Qwen (ÈÄö‰πâÂçÉÈóÆ) chat & pretrained large language model proposed by Alibaba Cloud.","repositories":[{"url":"https://github.com/QwenLM/Qwen","primary":true}]},{"category":"Models","homepage_url":"https://github.com/lucidrains/denoising-diffusion-pytorch","id":"models--foundation-model--lucidrains-denoising-diffusion-pytorch","logo":"logos/0b22fe6553ffd6c927de0a316a0b01cde01e4c3932170c13bfebb45153f0cdb8.svg","name":"lucidrains/denoising-diffusion-pytorch","subcategory":"Foundation model","website":"https://github.com/lucidrains/denoising-diffusion-pytorch","description":"Implementation of Denoising Diffusion Probabilistic Model in Pytorch","repositories":[{"url":"https://github.com/lucidrains/denoising-diffusion-pytorch","primary":true}]},{"category":"Models","homepage_url":"https://github.com/NVIDIA/Megatron-LM","id":"models--foundation-model--nvidia-megatron-lm","logo":"logos/0b22fe6553ffd6c927de0a316a0b01cde01e4c3932170c13bfebb45153f0cdb8.svg","name":"NVIDIA/Megatron-LM","subcategory":"Foundation model","website":"https://github.com/NVIDIA/Megatron-LM","description":"Ongoing research training transformer models at scale","repositories":[{"url":"https://github.com/NVIDIA/Megatron-LM","primary":true}]}]}